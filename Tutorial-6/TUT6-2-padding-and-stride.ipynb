{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 0,
        "id": "D6c2MjGWeElt"
      },
      "source": [
        "# Padding and Stride\n",
        "\n",
        "In the previous example of conv,\n",
        "our input is ($3$, $3$)\n",
        "and our conv kernel ($2$, $2$),\n",
        "yielding an output representation with ($2$, $2$).\n",
        "\n",
        "Assuming that\n",
        "the input shape is $n_h\\times n_w$\n",
        "and the convolution kernel shape is $k_h\\times k_w$,\n",
        "then the output shape will be\n",
        "$(n_h-k_h+1) \\times (n_w-k_w+1)$.\n",
        "\n",
        "Therefore, the output shape of the convolutional layer\n",
        "is determined by the shape of the input\n",
        "and the shape of the convolution kernel.\n",
        "\n",
        "Padding and strided convolutions will also \n",
        "affect the size of the output.\n",
        "\n",
        "As motivation, note that since kernels generally\n",
        "have width and height greater than $1$,\n",
        "after applying many successive convolutions,\n",
        "we tend to wind up with outputs that are\n",
        "considerably smaller than our input.\n",
        "*Padding* is the most popular tool for handling this issue.\n",
        "\n",
        "In other cases, we may want to reduce the dimensionality drastically,\n",
        "e.g., if we find the original input resolution to be unwieldy.\n",
        "*Strided convolutions* are a popular technique that can help in these instances.\n",
        "\n",
        "## Padding\n",
        "\n",
        "When applying many successive convolutional layers,\n",
        "we tend to lose pixels on the perimeter of our image.\n",
        "\n",
        "One straightforward solution to this problem\n",
        "is to add extra pixels of filler around the boundary of our input image,\n",
        "thus increasing the effective size of the image.\n",
        "Typically, we set the values of the extra pixels to zero.\n",
        "In the image, we pad a $3 \\times 3$ input,\n",
        "increasing its size to $5 \\times 5$.\n",
        "\n",
        "The corresponding output then increases to a $4 \\times 4$ matrix.\n",
        "The shaded portions are the first output element as well as the input and kernel tensor elements used for the output computation: $0\\times0+0\\times1+0\\times2+0\\times3=0$.\n",
        "\n",
        "![Two-dimensional cross-correlation with padding.](https://github.com/d2l-ai/d2l-pytorch-colab/blob/master/img/conv-pad.svg?raw=1)\n",
        "\n",
        "In general, if we add a total of $p_h$ rows of padding\n",
        "(roughly half on top and half on bottom)\n",
        "and a total of $p_w$ columns of padding\n",
        "(roughly half on the left and half on the right),\n",
        "the output shape will be\n",
        "\n",
        "$$(n_h-k_h+p_h+1)\\times(n_w-k_w+p_w+1).$$\n",
        "\n",
        "This means that the height and width of the output\n",
        "will increase by $p_h$ and $p_w$, respectively.\n",
        "\n",
        "In many cases, we will want to set $p_h=k_h-1$ and $p_w=k_w-1$\n",
        "to give the input and output the same height and width.\n",
        "\n",
        "Assuming that $k_h$ is odd here,\n",
        "we will pad $p_h/2$ rows on both sides of the height.\n",
        "If $k_h$ is even, one possibility is to\n",
        "pad $\\lceil p_h/2\\rceil$ rows on the top of the input\n",
        "and $\\lfloor p_h/2\\rfloor$ rows on the bottom.\n",
        "We will pad both sides of the width in the same way.\n",
        "\n",
        "CNNs commonly use convolution kernels\n",
        "with odd height and width values, such as 1, 3, 5, or 7.\n",
        "Choosing odd kernel sizes has the benefit\n",
        "that we can preserve the spatial dimensionality\n",
        "while padding with the same number of rows on top and bottom,\n",
        "and the same number of columns on left and right.\n",
        "\n",
        "In the following example, we create a two-dimensional convolutional layer\n",
        "with a height and width of 3\n",
        "and (**apply 1 pixel of padding on all sides.**)\n",
        "Given an input with a height and width of 8,\n",
        "we find that the height and width of the output is also 8.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 2,
        "tab": [
          "pytorch"
        ],
        "id": "xEiJW4lEeElx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# We define a convenience function to calculate the convolutional layer. This\n",
        "# function initializes the convolutional layer weights and performs\n",
        "# corresponding dimensionality elevations and reductions on the input and\n",
        "# output\n",
        "def comp_conv2d(conv2d, X):\n",
        "    # Here (1, 1) indicates that the batch size and the number of channels\n",
        "    # are both 1\n",
        "    print('X.shape: ')\n",
        "    print(X.shape)\n",
        "    X = X.reshape((1, 1) + X.shape)\n",
        "    Y = conv2d(X)\n",
        "    # Exclude the first two dimensions that do not interest us: examples and\n",
        "    # channels\n",
        "    print('Y.shape: ')\n",
        "    return Y.reshape(Y.shape[2:])\n",
        "# Note that here 1 row or column is padded on either side, so a total of 2\n",
        "# rows or columns are added\n",
        "conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1)\n",
        "X = torch.rand(size=(8, 8))\n",
        "comp_conv2d(conv2d, X).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 4,
        "id": "NqYfxjlceEly"
      },
      "source": [
        "When the height and width of the convolution kernel are different,\n",
        "we can make the output and input have the same height and width\n",
        "by [**setting different padding numbers for height and width.**]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 6,
        "tab": [
          "pytorch"
        ],
        "id": "cdpKWV4WeElz"
      },
      "outputs": [],
      "source": [
        "# Here, we use a convolution kernel with a height of 5 and a width of 3. The\n",
        "# padding numbers on either side of the height and width are 2 and 1,\n",
        "# respectively\n",
        "conv2d = nn.Conv2d(1, 1, kernel_size=(5, 3), padding=(2, 1))\n",
        "comp_conv2d(conv2d, X).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 8,
        "id": "N9ONawjFeElz"
      },
      "source": [
        "## Stride\n",
        "\n",
        "When computing the cross-correlation,\n",
        "we start with the convolution window\n",
        "at the upper-left corner of the input tensor,\n",
        "and then slide it over all locations both down and to the right.\n",
        "In previous examples, we default to sliding one element at a time.\n",
        "\n",
        "However, sometimes, either for computational efficiency\n",
        "or because we wish to downsample,\n",
        "we move our window more than one element at a time,\n",
        "skipping the intermediate locations.\n",
        "\n",
        "We refer to the number of rows and columns traversed per slide as the *stride*.\n",
        "So far, we have used strides of 1, both for height and width.\n",
        "\n",
        "The image shows a two-dimensional cross-correlation operation\n",
        "with a stride of 3 vertically and 2 horizontally.\n",
        "\n",
        "The shaded portions are the output elements as well as the input and kernel tensor elements used for the output computation: $0\\times0+0\\times1+1\\times2+2\\times3=8$, $0\\times0+6\\times1+0\\times2+0\\times3=6$.\n",
        "\n",
        "![Cross-correlation with strides of 3 and 2 for height and width, respectively.](https://github.com/d2l-ai/d2l-pytorch-colab/blob/master/img/conv-stride.svg?raw=1)\n",
        "\n",
        "In general, when the stride for the height is $s_h$\n",
        "and the stride for the width is $s_w$, the output shape is\n",
        "\n",
        "$$\\lfloor(n_h-k_h+p_h+s_h)/s_h\\rfloor \\times \\lfloor(n_w-k_w+p_w+s_w)/s_w\\rfloor.$$\n",
        "\n",
        "If we set $p_h=k_h-1$ and $p_w=k_w-1$,\n",
        "then the output shape will be simplified to\n",
        "$\\lfloor(n_h+s_h-1)/s_h\\rfloor \\times \\lfloor(n_w+s_w-1)/s_w\\rfloor$.\n",
        "\n",
        "Going a step further, if the input height and width\n",
        "are divisible by the strides on the height and width,\n",
        "then the output shape will be $(n_h/s_h) \\times (n_w/s_w)$.\n",
        "\n",
        "$n_h = 3, k_h = 2, p_h = 2, s_h = 3$\n",
        "\n",
        "$\\lfloor(n_h-k_h+p_h+s_h)/s_h\\rfloor = \\lfloor(3-2+2+3)/3\\rfloor = \\lfloor2\\rfloor = 2$\n",
        "\n",
        "$n_w = 3, k_w = 2, p_w = 2, s_w = 2$\n",
        "\n",
        "$\\lfloor(n_w-k_w+p_w+s_w)/s_w\\rfloor = \\lfloor(3-2+2+2)/2\\rfloor = \\lfloor2.5\\rfloor = 2$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 10,
        "tab": [
          "pytorch"
        ],
        "id": "lWIJSJnteElz"
      },
      "outputs": [],
      "source": [
        "conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1, stride=2)\n",
        "comp_conv2d(conv2d, X).shape # X: (8, 8)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$n_h = 8, k_h = 3, p_h = 1, s_h = 2$\n",
        "\n",
        "$\\lfloor(n_h-k_h+p_h+s_h)/s_h\\rfloor = \\lfloor(8-3+1+2)/2\\rfloor = \\lfloor4\\rfloor = 4$"
      ],
      "metadata": {
        "id": "lGIusysJDpy9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 12,
        "id": "zo8xMgYLeEl0"
      },
      "source": [
        "Next, we will look at (**a slightly more complicated example**).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 14,
        "tab": [
          "pytorch"
        ],
        "id": "PfJced7aeEl0"
      },
      "outputs": [],
      "source": [
        "conv2d = nn.Conv2d(1, 1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))\n",
        "comp_conv2d(conv2d, X).shape # X: (8, 8)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$n_h = 8, k_h = 3, p_h = 0, s_h = 3$\n",
        "\n",
        "$\\lfloor(n_h-k_h+p_h+s_h)/s_h\\rfloor = \\lfloor(8-3+0+3)/3\\rfloor = \\lfloor(8/3)\\rfloor = 2$\n",
        "\n",
        "$n_w = 8, k_w = 5, p_w = 1, s_w = 4$\n",
        "\n",
        "$\\lfloor(n_w-k_w+p_w+s_w)/s_w\\rfloor = \\lfloor(8-5+1+4)/4\\rfloor = \\lfloor2\\rfloor = 2$"
      ],
      "metadata": {
        "id": "8eTGg6F2DtbM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 16,
        "id": "piW2g7jveEl0"
      },
      "source": [
        "For the sake of brevity, when the padding number\n",
        "on both sides of the input height and width are $p_h$ and $p_w$ respectively, we call the padding $(p_h, p_w)$.\n",
        "Specifically, when $p_h = p_w = p$, the padding is $p$.\n",
        "\n",
        "When the strides on the height and width are $s_h$ and $s_w$, respectively,\n",
        "we call the stride $(s_h, s_w)$.\n",
        "Specifically, when $s_h = s_w = s$, the stride is $s$.\n",
        "\n",
        "By default, the padding is 0 and the stride is 1.\n",
        "\n",
        "In practice, we rarely use inhomogeneous strides or padding,\n",
        "i.e., we usually have $p_h = p_w$ and $s_h = s_w$.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "name": "TUT6-2-padding-and-stride.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}